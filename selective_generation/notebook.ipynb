{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a023acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as fin:\n",
    "    config = yaml.load(fin, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66bedde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>LTC</th>\n",
       "      <th>LTO</th>\n",
       "      <th>TED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joint-sequence, length-normalized entropy from importance weighting beam search with beam of 20</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.7276</td>\n",
       "      <td>0.6786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joint-sequence, length normalized entropy from 100 ancestral samples</td>\n",
       "      <td>0.6403</td>\n",
       "      <td>0.6438</td>\n",
       "      <td>0.5377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joint-sequence, length normalized entropy from 100 nucleus samples (p=0.9)</td>\n",
       "      <td>0.6984</td>\n",
       "      <td>0.7218</td>\n",
       "      <td>0.6481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Average word error rate between hypothesis and 100 ancestral samples</td>\n",
       "      <td>0.5824</td>\n",
       "      <td>0.6676</td>\n",
       "      <td>0.7168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Average word error rate between hypothesis and 100 nucleus samples (p=0.9)</td>\n",
       "      <td>0.7026</td>\n",
       "      <td>0.7491</td>\n",
       "      <td>0.7177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Negative mean of the token probabilities of the predicted transcription</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>0.7260</td>\n",
       "      <td>0.6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Negative sum of the token probabilities of the predicted transcription</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>0.3459</td>\n",
       "      <td>0.1690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            Method  \\\n",
       "0  Joint-sequence, length-normalized entropy from importance weighting beam search with beam of 20   \n",
       "1                             Joint-sequence, length normalized entropy from 100 ancestral samples   \n",
       "2                       Joint-sequence, length normalized entropy from 100 nucleus samples (p=0.9)   \n",
       "3                             Average word error rate between hypothesis and 100 ancestral samples   \n",
       "4                       Average word error rate between hypothesis and 100 nucleus samples (p=0.9)   \n",
       "5                          Negative mean of the token probabilities of the predicted transcription   \n",
       "6                           Negative sum of the token probabilities of the predicted transcription   \n",
       "\n",
       "      LTC     LTO     TED  \n",
       "0  0.7041  0.7276  0.6786  \n",
       "1  0.6403  0.6438  0.5377  \n",
       "2  0.6984  0.7218  0.6481  \n",
       "3  0.5824  0.6676  0.7168  \n",
       "4  0.7026  0.7491  0.7177  \n",
       "5  0.7015  0.7260  0.6701  \n",
       "6  0.2964  0.3459  0.1690  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = [f for f in os.listdir(config[\"results_dir\"]) if f.startswith(\"metrics_\")]\n",
    "\n",
    "values_map = {}\n",
    "for fname in fnames:\n",
    "    input_file = os.path.join(config[\"results_dir\"], fname)\n",
    "    m = pd.read_csv(input_file, header=None)\n",
    "    m.columns = [\"metric\", \"value\"]\n",
    "    dataset = fname.split(\"_\")[1]\n",
    "    topp = fname.split(\"_\")[-2]\n",
    "    for i in range(len(m)):\n",
    "        metric = m[\"metric\"].values[i]\n",
    "        if metric == \"mean_wer\":\n",
    "            continue\n",
    "        value = m[\"value\"].values[i]\n",
    "        key = (metric, dataset, topp)\n",
    "        assert key not in values_map\n",
    "        values_map[key] = value\n",
    "\n",
    "order = [\n",
    "    (\"prr_hypo_entropy\", \"topp1.0\",\n",
    "     \"Joint-sequence, length-normalized entropy from importance weighting beam search with beam of 20\"),\n",
    "    (\"prr_samp_entropy\", \"topp1.0\",\n",
    "     \"Joint-sequence, length normalized entropy from 100 ancestral samples\"),\n",
    "    (\"prr_samp_entropy\", \"topp0.9\",\n",
    "     \"Joint-sequence, length normalized entropy from 100 nucleus samples (p=0.9)\"),\n",
    "    (\"prr_mean_samp_wers\", \"topp1.0\",\n",
    "     \"Average word error rate between hypothesis and 100 ancestral samples\"),\n",
    "    (\"prr_mean_samp_wers\", \"topp0.9\",\n",
    "     \"Average word error rate between hypothesis and 100 nucleus samples (p=0.9)\"),\n",
    "    (\"prr_neg_mean_token_logprobs\", \"topp1.0\",\n",
    "     \"Negative mean of the token probabilities of the predicted transcription\"),\n",
    "    (\"prr_neg_sum_token_logprobs\", \"topp1.0\",\n",
    "     \"Negative sum of the token probabilities of the predicted transcription\")\n",
    "]\n",
    "table = []\n",
    "for metric, topp, desc in order:\n",
    "    row = {}\n",
    "    row[\"Method\"] = desc\n",
    "    for dataset, name in [(\"librispeechclean\", \"LTC\"), (\"librispeechother\", \"LTO\"), (\"tedlium\", \"TED\")]:\n",
    "        row[name] = np.round(values_map[(metric, dataset, topp)], 4)\n",
    "    table.append(row)\n",
    "df = pd.DataFrame(table)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f008e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom",
   "language": "python",
   "name": "custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
